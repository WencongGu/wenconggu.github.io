---
title: 信息熵简述
date: 2024-01-06 23:08:52
tags: Math
mathjax: true
---

在信息论中，熵是接收的每条消息中包含的信息的平均量，又被称为信息熵、信源熵、平均自信息量。这里，“消息”代表来自分布或数据流中的事件、样本或特征。熵的单位通常为比特，但也用Sh、nat、Hart计量，取决于定义用到对数的底。

<!--more-->

这是一个不大不小的话题。

## 从零论述信息熵

信息是什么？信息的核心在于**消除不确定性**。为了确定一个具有多种状态系统的当前状态，就需要信息的传递。传递的信息有大有小，信息熵就是用来衡量信息大小的概念。

### 从简单例子说起

比如抛掷一枚硬币，落地后，未观察前，硬币的正反有两种等可能的状态。当观测硬币后，就消除了两种可能状态的不确定性，这一行为就传递了信息。

信息在传输时需要编码。我们想要将硬币的正反状态的结果传递出去，就要使用某种信息载体。人类语言也是一种信息载体，比如可以直接说“正”、“反”，就足以传递出所需要的所有信息。为了一般化，不妨使用0和1来表示，可以约定0代表正，1代表反。

在面对更加一般的系统时，人类语言携带的信息往往有很大冗余，这使得无法类似地衡量确定系统状态所需的信息量，准确的说是**最小**的信息量。这个“最小”的意义还在于回答类似这样的实际问题：将一个文件按某种方式进行无损压缩，这个压缩后文件大小最小是多小？

通信过程中所使用的信息载体是单一的，比如使用0、1，或者“正”、“反”。在一个通信过程中，要使用这个最小单元对系统状态作出描述。一个单元代表的状态显然是有限的，比如0-1系统，也即现代计算机的二进制存储，所以我们需要很多个单元的组合来准确描述一个状态。什么是“准确”？意味着每种单元的组合都准确地对应着一种系统状态。比如抛硬币的问题，使用一个单元足矣；但是对于两枚硬币的状态，有四种情况，那么需要4个单元。

可以简单地分析出，对于有 $n$ 个**等可能状态**的系统，使用**二进制数字单位**准确描述其状态需要的单元数 $h$ 要满足 $2^h\ge n$，因此 $h\ge \log_2{n}$，也就是**最少**需要 $\log_2{n}$ 个单位，就说这个系统所需要的信息量 $S=\log_2{n}$。

这就是这种简单情况下的信息熵的表征，它对“信息”这一抽象概念做了定量描述，刻画了确定一个系统状态所需的最低通信或者存储成本。

### 更一般的

上面的例子的确太简单了。

一方面，我们可能有多种选择，而不是仅仅只能使用二进制数做编码单位。这一问题很好解决，对于使用有 $p$ 个状态的信息单位，描述相同的系统所需的信息量就是 $S=\log_p{n}$，当 $p=2$ 时，这个单位取名为**比特**，**bit**，$p$ 也可能取其他数，所得不尽相同，但是并无本质不同。

另一方面，“有 $n$ 个**等可能状态**” 这一条件也显得十分苛刻，我们面对的系统大都不是等可能。针对这个问题，处理办法是将其“转化”为等可能状态的系统。

比如一枚硬币正面朝上的概率是 $0.8$，反面朝上概率是 $0.2$。反面朝上概率为 $0.2$ 可以被描述为从一个黑盒中摸球，这个黑盒中有 $\frac{1}{0.2}=5$ 个白球，$1$ 个黑球，摸到黑球的概率；类似的，正面朝上的黑盒中有 $\frac{1}{0.8}=1.25$ 个白球，$1$ 个黑球。两个系统出现的概率分别为它们对应的概率，所需要的信息量就是两个系统信息量之和。因此就可以计算出整个系统的信息量为 $S=0.2\log_2\frac{1}{0.2}+0.8\log_2\frac{1}{0.8}$。

可以轻松地推广到一般系统中。对于具有如下概率分布的系统：

|State|1|2|$\cdots$|n
|:-:|:-:|:-:|:-:|:-:|
|Probability|$p_1$|$p_2$|$\cdots$|$p_n$|

其信息量为：

$$S=-\sum_{i=1}^{n}p_i\log p_i$$

热力学中有熵的定义（玻尔兹曼熵）：

$$S\propto \ln \Omega$$

这个信息量就是**信息熵**。























<!-- 一个黑箱中装有 $8$ 个白球和 $2$ 个黑球，每次抛硬币都相当于随机从箱子中摸出一个球，而每个球被摸到的概率是均等的，摸到白球对应硬币正面朝上，摸到黑球对应反面朝上。 -->